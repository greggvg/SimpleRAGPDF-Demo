{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae048f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary frameworks and libraries\n",
    "!pip -q install pymupdf sentence-transformers faiss-cpu transformers accelerate torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955ea4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from google.colab import files\n",
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Upload PDF\n",
    "uploaded = files.upload()\n",
    "pdf_file = list(uploaded.keys())[0]\n",
    "print(\"Uploaded:\", pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d017f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split PDF into Chunks\n",
    "chunks = recursive_text_splitter(raw_text, chunk_size=900, chunk_overlap=150)\n",
    "print(\"Chunks:\", len(chunks))\n",
    "print(\"Sample chunk:\\n\", chunks[0][:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896ca228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Loop\n",
    "while True:\n",
    "    query = input(\"\\nAsk a question (type 'exit' to stop): \").strip()\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    print(\"\\nAnswer:\\n\")\n",
    "    print(ask_question(query, top_k=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afbdc5d",
   "metadata": {},
   "source": [
    "## Interactive Query Loop\n",
    "\n",
    "Ask multiple questions in an interactive loop. Type 'exit' to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c5cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Once\n",
    "print(ask_question(\"What is this document about?\", top_k=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ef2618",
   "metadata": {},
   "source": [
    "## Test the RAG System\n",
    "\n",
    "Ask a sample question to test the complete RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4668d9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval + Answer Function\n",
    "def retrieve_context(question: str, top_k: int = 3) -> str:\n",
    "    q_emb = embedding_model.encode([question], convert_to_numpy=True)\n",
    "    distances, indices = index.search(q_emb, top_k)\n",
    "    selected = [chunks[i] for i in indices[0]]\n",
    "    return \"\\n\\n\".join(selected)\n",
    "\n",
    "def ask_question(\n",
    "    question: str,\n",
    "    top_k: int = 3,\n",
    "    max_input_tokens: int = 512,\n",
    "    max_new_tokens: int = 120\n",
    ") -> str:\n",
    "    t0 = time.time()\n",
    "\n",
    "    context = retrieve_context(question, top_k=top_k)\n",
    "\n",
    "    prompt = (\n",
    "        \"Use the context to answer the question.\\n\"\n",
    "        \"If the answer is not in the context, say: I do not know.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question:\\n{question}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_tokens\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=False,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "\n",
    "    t2 = time.time()\n",
    "\n",
    "    gen_ids = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "    answer = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    print(\"Timing, tokenize:\", round(t1 - t0, 2), \"s, generate:\", round(t2 - t1, 2), \"s\")\n",
    "    print(\"Input tokens:\", inputs[\"input_ids\"].shape[-1])\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbe553f",
   "metadata": {},
   "source": [
    "## Retrieval + Question Answering Functions\n",
    "\n",
    "Implement functions to retrieve relevant context and generate answers using the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f02905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a Small LLM (Stable Default)\n",
    "model_name = \"distilgpt2\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Model loaded on:\", next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ead2d89",
   "metadata": {},
   "source": [
    "## Load Language Model\n",
    "\n",
    "Load the distilgpt2 model and configure it for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e7b120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Embeddings and FAISS Index\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "t0 = time.time()\n",
    "emb = embedding_model.encode(chunks, convert_to_numpy=True, show_progress_bar=True)\n",
    "t1 = time.time()\n",
    "\n",
    "dim = emb.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(emb)\n",
    "\n",
    "print(\"Embeddings shape:\", emb.shape)\n",
    "print(\"FAISS vectors:\", index.ntotal)\n",
    "print(\"Embedding time (s):\", round(t1 - t0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652fc910",
   "metadata": {},
   "source": [
    "## Build Embeddings and FAISS Index\n",
    "\n",
    "Generate embeddings for all chunks and create a FAISS index for fast similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabbe2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split_by_separator(text: str, sep: str) -> List[str]:\n",
    "    if sep == \"\":\n",
    "        return list(text)\n",
    "    parts = text.split(sep)\n",
    "    out = []\n",
    "    for i, p in enumerate(parts):\n",
    "        if i < len(parts) - 1:\n",
    "            out.append(p + sep)\n",
    "        else:\n",
    "            out.append(p)\n",
    "    return out\n",
    "\n",
    "def recursive_text_splitter(\n",
    "    text: str,\n",
    "    chunk_size: int = 900,\n",
    "    chunk_overlap: int = 150,\n",
    "    separators: List[str] = None,\n",
    "    min_chunk_chars: int = 200\n",
    ") -> List[str]:\n",
    "    if separators is None:\n",
    "        separators = [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "\n",
    "    text = (text or \"\").strip()\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    def _recurse(t: str, seps: List[str]) -> List[str]:\n",
    "        if len(t) <= chunk_size or not seps:\n",
    "            return [t]\n",
    "\n",
    "        sep = seps[0]\n",
    "        parts = _split_by_separator(t, sep)\n",
    "\n",
    "        if len(parts) == 1:\n",
    "            return _recurse(t, seps[1:])\n",
    "\n",
    "        out, buf = [], \"\"\n",
    "        for part in parts:\n",
    "            if len(part) > chunk_size and seps[1:]:\n",
    "                if buf.strip():\n",
    "                    out.append(buf)\n",
    "                    buf = \"\"\n",
    "                out.extend(_recurse(part, seps[1:]))\n",
    "                continue\n",
    "\n",
    "            if len(buf) + len(part) <= chunk_size:\n",
    "                buf += part\n",
    "            else:\n",
    "                if buf.strip():\n",
    "                    out.append(buf)\n",
    "                buf = part\n",
    "\n",
    "        if buf.strip():\n",
    "            out.append(buf)\n",
    "\n",
    "        return out\n",
    "\n",
    "    pieces = _recurse(text, separators)\n",
    "\n",
    "    cleaned = []\n",
    "    for p in pieces:\n",
    "        p = p.strip()\n",
    "        if not p:\n",
    "            continue\n",
    "        if cleaned and len(p) < min_chunk_chars:\n",
    "            cleaned[-1] = (cleaned[-1].rstrip() + \" \" + p).strip()\n",
    "        else:\n",
    "            cleaned.append(p)\n",
    "\n",
    "    chunks = []\n",
    "    for p in cleaned:\n",
    "        if not chunks:\n",
    "            chunks.append(p)\n",
    "            continue\n",
    "        overlap_text = chunks[-1][-chunk_overlap:] if chunk_overlap > 0 else \"\"\n",
    "        merged = (overlap_text + p).strip()\n",
    "        if len(merged) > chunk_size:\n",
    "            merged = merged[-chunk_size:]\n",
    "        chunks.append(merged)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b023308b",
   "metadata": {},
   "source": [
    "## Custom Text Splitter (No LangChain)\n",
    "\n",
    "Implement a recursive text splitter that chunks text intelligently while preserving content structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2b0bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Text from PDF (PyMuPDF)\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages = []\n",
    "    for i in range(len(doc)):\n",
    "        pages.append(doc.load_page(i).get_text())\n",
    "    return \"\\n\".join(pages)\n",
    "\n",
    "raw_text = extract_text_from_pdf(pdf_file).strip()\n",
    "print(\"Extracted characters:\", len(raw_text))\n",
    "print(raw_text[:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3651b301",
   "metadata": {},
   "source": [
    "## Upload and Extract PDF Text\n",
    "\n",
    "Import required modules and upload a PDF file to extract text from it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a0f90a",
   "metadata": {},
   "source": [
    "# SimpleRAGPDF - Retrieval-Augmented Generation for PDFs\n",
    "\n",
    "This notebook implements a complete RAG system for PDF documents using:\n",
    "- **PyMuPDF** for PDF text extraction\n",
    "- **Sentence Transformers** for embeddings\n",
    "- **FAISS** for vector search\n",
    "- **Hugging Face Transformers** for generation (distilgpt2)\n",
    "\n",
    "No LangChain required!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
